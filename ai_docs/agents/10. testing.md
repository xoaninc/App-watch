# Testing Agent

Run and analyze tests, identify coverage gaps, and ensure code is properly tested.

## Purpose

Ensure code has adequate test coverage, tests pass, and testing best practices are followed.

## When to Use

Run this agent:
- After implementing a feature
- Before code review
- When tests fail in CI
- To identify coverage gaps
- When refactoring code

## Prerequisites

Understand the testing pyramid:
```
        /\
       /  \      E2E Tests (few)
      /----\
     /      \    Integration Tests (some)
    /--------\
   /          \  Unit Tests (many)
  --------------
```

## Instructions

### Step 1: Identify Test Requirements

Based on the code changes:
- What unit tests are needed?
- What integration tests are needed?
- What feature/E2E tests are needed?

### Step 2: Run Existing Tests

```bash
# Run all tests
[test command based on stack]

# Run with coverage
[coverage command]

# Run specific tests
[specific test command]
```

### Step 3: Analyze Results

For each failing test:
1. Identify the failure reason
2. Determine if it's a test bug or code bug
3. Suggest fix

### Step 4: Coverage Analysis

Check:
- Overall coverage percentage
- Coverage of new/changed code
- Uncovered critical paths

### Step 5: Test Quality Review

Verify tests:
- [ ] Test one thing each
- [ ] Have descriptive names
- [ ] Follow AAA pattern (Arrange, Act, Assert)
- [ ] Don't test implementation details
- [ ] Are deterministic (no flaky tests)
- [ ] Don't depend on external services

### Step 6: Missing Tests Identification

Identify untested:
- Code paths
- Edge cases
- Error scenarios
- Boundary conditions

## Output Format

```markdown
# Testing Report

**Date:** [date]
**Status:** PASS | FAIL
**Coverage:** [X]%

---

## Summary

| Category | Total | Passed | Failed | Skipped |
|----------|-------|--------|--------|---------|
| Unit | X | X | X | X |
| Integration | X | X | X | X |
| Feature/E2E | X | X | X | X |
| **Total** | **X** | **X** | **X** | **X** |

---

## Test Results

### Passed Tests: [count]
All passing tests executed successfully.

### Failed Tests: [count]

#### Test 1: [test name]

**File:** `[test file path]`
**Type:** Unit | Integration | Feature

**Failure:**
```
[error message / stack trace]
```

**Analysis:**
- **Cause:** [test bug / code bug / environment issue]
- **Root Cause:** [detailed explanation]

**Fix:**
```
[code fix or test fix]
```

---

#### Test 2: [test name]
[...]

---

### Skipped Tests: [count]

| Test | Reason |
|------|--------|
| [test] | [reason for skip] |

---

## Coverage Report

### Overall Coverage

| Metric | Current | Target | Status |
|--------|---------|--------|--------|
| Lines | X% | 80% | OK/WARN |
| Functions | X% | 80% | OK/WARN |
| Branches | X% | 70% | OK/WARN |

### Coverage by Module

| Module | Lines | Functions | Branches |
|--------|-------|-----------|----------|
| [module] | X% | X% | X% |
| [module] | X% | X% | X% |

### New/Changed Code Coverage

| File | Changed Lines | Covered | Coverage |
|------|---------------|---------|----------|
| [file] | X | Y | Z% |

### Uncovered Critical Paths

| File | Lines | Reason Critical |
|------|-------|-----------------|
| [file] | [lines] | [reason] |

---

## Missing Tests

### High Priority (Must Add)

#### 1. [Test Description]

**For:** `[file:function/method]`
**Type:** Unit | Integration

**Test Case:**
```
// Test name: should [expected behavior]
// Arrange
[setup code]

// Act
[action code]

// Assert
[assertion code]
```

**Why Critical:**
[Reason this test is important]

---

#### 2. [Test Description]
[...]

---

### Medium Priority (Should Add)

| Component | Missing Test | Type |
|-----------|--------------|------|
| [component] | [test case] | Unit |
| [component] | [test case] | Integration |

### Low Priority (Nice to Have)

| Component | Missing Test | Type |
|-----------|--------------|------|
| [component] | [edge case] | Unit |

---

## Test Quality Issues

### Anti-Patterns Found

#### 1. Testing Implementation Details

**File:** `[test file]`
**Issue:** Test verifies internal implementation instead of behavior

**Current:**
```
// Tests that specific method was called
assert(mock.methodX.called)
```

**Better:**
```
// Tests the outcome/behavior
assert(result.status === 'success')
```

---

#### 2. Multiple Assertions Testing Different Things

**File:** `[test file]`
**Issue:** One test verifying multiple unrelated things

**Recommendation:** Split into multiple focused tests

---

#### 3. Non-Deterministic Test

**File:** `[test file]`
**Issue:** Test depends on current time/random values

**Fix:** Mock time/random generators

---

### Naming Issues

| Test | Current Name | Suggested Name |
|------|--------------|----------------|
| [test] | `testCreate` | `should_create_user_when_valid_data_provided` |
| [test] | `test1` | `should_throw_when_email_invalid` |

---

## Test Recommendations

### New Tests to Write

```markdown
## Unit Tests

### [Class/Module Name]

1. **should_[expected_behavior]_when_[condition]**
   - Input: [input]
   - Expected: [output]
   - Edge case: [edge case if any]

2. **should_throw_[exception]_when_[invalid_condition]**
   - Input: [invalid input]
   - Expected: [exception type]

### [Next Class/Module]
[...]
```

### Integration Tests

```markdown
1. **[Feature] - Happy Path**
   - Setup: [setup steps]
   - Action: [action]
   - Verify: [database state, events published, etc.]

2. **[Feature] - Error Handling**
   - Setup: [setup with invalid state]
   - Action: [action]
   - Verify: [error handling, rollback, etc.]
```

---

## Test Commands

### Run All Tests
```bash
[command]
```

### Run Specific Test File
```bash
[command] [path]
```

### Run with Coverage
```bash
[command] --coverage
```

### Run Failed Tests Only
```bash
[command] --failed
```

### Watch Mode
```bash
[command] --watch
```

---

## CI/CD Configuration

```yaml
test:
  runs-on: ubuntu-latest
  steps:
    - name: Run Tests
      run: [test command]

    - name: Upload Coverage
      uses: codecov/codecov-action@v3

    - name: Check Coverage Threshold
      run: |
        [coverage check command]
```

---

## Next Steps

### If PASS:
- [ ] Review coverage report
- [ ] Add any missing high-priority tests
- [ ] Proceed to code review

### If FAIL:
1. Fix failing tests
2. Re-run test suite
3. Verify all pass
4. Add missing coverage
```

## Testing Patterns by Layer

### Domain Layer (Unit Tests)

```
// Entity test
test should_create_entity_with_valid_data:
    // Arrange
    id = EntityId.random()

    // Act
    entity = Entity.create(id, ...)

    // Assert
    assert(entity.id === id)
    assert(entity.status === 'draft')

test should_throw_when_invalid_transition:
    // Arrange
    entity = createCompletedEntity()

    // Act & Assert
    assertThrows(() => entity.activate())
```

### Application Layer (Unit + Integration)

```
// Handler test
test handler_should_create_entity:
    // Arrange
    repository = mock(RepositoryInterface)
    handler = new CreateHandler(repository)
    command = new CreateCommand(...)

    // Act
    handler.invoke(command)

    // Assert
    repository.shouldHaveReceived('save').once()
```

### HTTP Layer (Feature Tests)

```
// API test
test POST_entities_should_return_201:
    // Arrange
    data = { name: 'Test', ... }

    // Act
    response = post('/api/entities', data)

    // Assert
    assert(response.status === 201)
    assert(response.body.name === 'Test')
    assertDatabaseHas('entities', { name: 'Test' })
```

## Tips

- Write tests first (TDD) for complex logic
- Use factories for test data
- Mock external dependencies
- Keep tests fast
- Test behavior, not implementation
- One assertion per concept (can be multiple asserts)
- Use descriptive test names
